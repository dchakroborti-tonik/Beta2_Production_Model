{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcfa471-76f1-4040-9f52-ca4ab1be1007",
   "metadata": {},
   "source": [
    "# <div align=\"center\" style=\"color: #ff5733;\">Income Estimation Regression Model (Catboost) Beta 2 STEP 2 Production Model</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d758a-33be-48fb-9861-74e09970c5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"Set all seeds and environment variables for reproducibility\"\"\"\n",
    "    import os\n",
    "    # Set environment variables before any other imports\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    # Then set other seeds\n",
    "    import numpy as np\n",
    "    import random\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Force single-thread operations\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    os.environ['MKL_NUM_THREADS'] = '1'\n",
    "    os.environ['OPENBLAS_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58298b4e-5670-4c23-999c-070383ed54dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First cell of your notebook\n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea12d7f-f41e-4433-9a73-ccd4c5fc798c",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673edb0-82f6-4622-b5e2-37eca8413126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, \n",
    "    mean_squared_error, \n",
    "    r2_score, \n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import catboost as cb\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import shap\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from typing import Union, List\n",
    "from scipy.stats import mstats\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from fuzzywuzzy import fuzz\n",
    "import joblib\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "# Connection to Bigquery\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Settings in this Notebook\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8ba51-e8c5-4e0b-a180-bdaa6f6cd08d",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c496a1-2eb3-430e-8d3d-40d44201a95f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "BUCKET_NAME = \"prod-asia-southeast1-tonik-aiml-workspace\"\n",
    "CLOUDPATH = \"Monthly_Income_Estimation/Beta2_Step2_Production_Model\"\n",
    "CLOUDPATH_DATA = \"Monthly_Income_Estimation/Beta2_Step2_Production_Model/Data\"\n",
    "CLOUDPATH_ARTIFACTS = \"Monthly_Income_Estimation/Beta2_Step2_Production_Model/Artifacts\"\n",
    "CLOUDPATH_TARGET = \"Monthly_Income_Estimation/Target_Encoded_Artifacts\"\n",
    "DATATYPE = \"Step2\"\n",
    "LOCALPATH = \"/home/jupyter/Models/Beta2_Production_Model/Beta2_Production_Model/Data/\"\n",
    "LOCALPATHSRC = \"/home/jupyter/Models/Beta2_Production_Model/Beta2_Production_Model/src/\"\n",
    "LOCALPATHARTIFACTS = \"/home/jupyter/Models/Beta2_Production_Model/Beta2_Production_Model/Artifacts/\"\n",
    "MODELNAME = \"Beta2\"\n",
    "VERSIONNAME = \"1_0\"\n",
    "PRODUCT_TYPE = 'SIL_Quick'\n",
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94f1bb-920b-4b85-b97e-9fafff596a96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40306c-5d64-49f1-a040-f79bfbd43f02",
   "metadata": {},
   "source": [
    "## dfdescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eca27f-a69c-4a09-bbf3-7f07cb44614e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dfdescription(df):\n",
    "    print(f\"The shape of the data frame is :\\t {df.shape}\")\n",
    "    print(f\"The data types of columns in dataframe is: \\n{df.dtypes}\")\n",
    "    print(f\"The description of numerical columns is:\\t {df.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f858e91-436f-4b37-9e69-8f1db7489d67",
   "metadata": {},
   "source": [
    "## add_column_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edea88-0e5c-48dc-a008-61afea7c7f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_prefix(df: pd.DataFrame, \n",
    "                      prefix: str, \n",
    "                      columns: Union[str, List[str]] = None):\n",
    "    \"\"\"\n",
    "    Add a prefix to specified columns in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame whose columns need to be renamed\n",
    "    prefix : str\n",
    "        The prefix to be added to selected column names\n",
    "    columns : str or list of str, optional\n",
    "        The specific column(s) to add prefix to. \n",
    "        If None, applies prefix to all columns.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A new DataFrame with prefixed column names\n",
    "    \n",
    "    Examples:\n",
    "    ---------\n",
    "    >>> data = pd.DataFrame({\n",
    "    ...     'name': ['Alice', 'Bob'], \n",
    "    ...     'age': [25, 30], \n",
    "    ...     'city': ['New York', 'San Francisco']\n",
    "    ... })\n",
    "    >>> \n",
    "    >>> # Add prefix to specific columns\n",
    "    >>> prefixed_data = add_column_prefix(data, 'user_', ['name', 'age'])\n",
    "    >>> print(prefixed_data.columns)\n",
    "    Index(['user_name', 'user_age', 'city'], dtype='object')\n",
    "    \n",
    "    >>> # Add prefix to all columns\n",
    "    >>> all_prefixed = add_column_prefix(data, 'user_')\n",
    "    >>> print(all_prefixed.columns)\n",
    "    Index(['user_name', 'user_age', 'user_city'], dtype='object')\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # If no specific columns are provided, use all columns\n",
    "    if columns is None:\n",
    "        columns = df.columns.tolist()\n",
    "    \n",
    "    # Ensure columns is a list\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Validate that specified columns exist in the DataFrame\n",
    "    invalid_columns = set(columns) - set(df.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {invalid_columns}\")\n",
    "    \n",
    "    # Create a dictionary to map selected column names to new column names\n",
    "    rename_dict = {col: f\"{prefix}{col}\" for col in columns}\n",
    "    \n",
    "    # Rename the specified columns\n",
    "    df_copy.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ce09a-72f8-4d1b-8a45-ea63f6639559",
   "metadata": {},
   "source": [
    "## Upload_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78a429-c82d-48b9-8dec-8ae39fee2526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket. Â  \n",
    "\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the bucket.\n",
    "        source_file_name: The path to the file to upload.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4240f23e-9824-4895-8a4c-9185f4d15f5b",
   "metadata": {},
   "source": [
    "## upload_model_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd0880-a242-4dc7-83a8-f68a8ffdff3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to GCS\n",
    "def upload_model_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a model to a GCS bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_file_name: The path to the local model file.\n",
    "        destination_blob_name: The name of the blob to be created in GCS.\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f631e7b-990d-4a39-8fe0-4fd5a4e3efb6",
   "metadata": {},
   "source": [
    "## plot_actual_vs_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429e969-83f8-4d3f-a540-b66f3b50eadb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, title='Actual vs Predicted Values'):\n",
    "    \"\"\"\n",
    "    Create a scatter plot of actual vs predicted values\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425787ff-bc44-422e-a6f4-0ff4e1370050",
   "metadata": {},
   "source": [
    "## plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1d98e-200d-4845-8aa6-1eb844e43a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, title='Residual Plot'):\n",
    "    \"\"\"\n",
    "    Create a residual plot to visualize model errors with type conversion\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert to numpy float arrays to ensure type compatibility\n",
    "    y_true_float = np.array(y_true, dtype=float)\n",
    "    y_pred_float = np.array(y_pred, dtype=float)\n",
    "    \n",
    "    residuals = y_true_float - y_pred_float\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_float, residuals, alpha=0.5)\n",
    "    plt.hlines(y=0, xmin=y_pred_float.min(), xmax=y_pred_float.max(), color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c5933-8fcb-49c0-8f7e-26b964e370a6",
   "metadata": {},
   "source": [
    "## plot_residuals_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00440c-38dc-420b-b65f-3e407ce52efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_residuals_hist(y_true, y_pred, title='Residual Histogram Plot'):\n",
    "    \"\"\"\n",
    "    Create a residual plot to visualize model errors with type conversion\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: True target values\n",
    "    - y_pred: Predicted target values\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    # Convert to numpy float arrays to ensure type compatibility\n",
    "    y_true_float = np.array(y_true, dtype=float)\n",
    "    y_pred_float = np.array(y_pred, dtype=float)\n",
    "    \n",
    "    residuals = y_true_float - y_pred_float\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title('Residual Histogram')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9e4bd-7b2d-4174-b986-5ce1377f81f9",
   "metadata": {},
   "source": [
    "## plot_lift_chart  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f07524-1aff-48f9-a645-f3e14466b10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_lift_chart(y_test, y_pred, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plots a lift chart for a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (array-like): Actual target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "        n_bins (int): Number of bins/quantiles to group the data (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays the lift chart.\n",
    "    \"\"\"\n",
    "    # Combine actual and predicted values into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Create quantile-based bins\n",
    "    results['Decile'] = pd.qcut(results['Predicted'], q=n_bins, labels=False)\n",
    "\n",
    "    # Group by decile and calculate mean actual and predicted values\n",
    "    lift_chart_data = results.groupby('Decile').agg(\n",
    "        Avg_Predicted=('Predicted', 'mean'),\n",
    "        Avg_Actual=('Actual', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Plot the lift chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lift_chart_data['Avg_Predicted'], label='Predicted', marker='o')\n",
    "    plt.plot(lift_chart_data['Avg_Actual'], label='Actual', marker='s')\n",
    "    plt.title(\"Lift Chart\")\n",
    "    plt.xlabel(f\"Decile (1-{n_bins})\")\n",
    "    plt.ylabel(\"Average Value\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9c802-8838-4ba9-be11-9a252b9839e0",
   "metadata": {},
   "source": [
    "## plot_gain_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba445788-587b-46ae-a1b9-4e6c84008ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_gain_chart(y_test, y_pred, n_bins=10):\n",
    "    \"\"\"\n",
    "    Plots a gain chart for a regression model.\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (array-like): Actual target values.\n",
    "        y_pred (array-like): Predicted target values.\n",
    "        n_bins (int): Number of bins/quantiles to group the data (default: 10).\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays the gain chart.\n",
    "    \"\"\"\n",
    "    # Combine actual and predicted values into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "\n",
    "    # Sort by predicted values\n",
    "    results = results.sort_values(by='Predicted', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Calculate cumulative sums for actual and predicted values\n",
    "    results['Cumulative_Actual'] = results['Actual'].cumsum()\n",
    "    results['Cumulative_Predicted'] = results['Predicted'].cumsum()\n",
    "\n",
    "    # Normalize cumulative sums to percentage of total\n",
    "    results['Cumulative_Actual_Percent'] = results['Cumulative_Actual'] / results['Actual'].sum() * 100\n",
    "    results['Cumulative_Predicted_Percent'] = results['Cumulative_Predicted'] / results['Predicted'].sum() * 100\n",
    "    results['Percentage_of_Data'] = np.linspace(1 / len(results), 1, len(results)) * 100\n",
    "\n",
    "    # Plot the gain chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['Percentage_of_Data'], results['Cumulative_Predicted_Percent'], label='Predicted', marker='o')\n",
    "    plt.plot(results['Percentage_of_Data'], results['Cumulative_Actual_Percent'], label='Actual', marker='s')\n",
    "    plt.title(\"Gain Chart\")\n",
    "    plt.xlabel(\"Percentage of Data\")\n",
    "    plt.ylabel(\"Cumulative Percentage\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948b817-9647-4b50-b309-a23d089c1b7c",
   "metadata": {},
   "source": [
    "## save_df_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc4b70-89a6-45fd-897b-59dd74722046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120c140-c224-4635-9806-53ea15c62340",
   "metadata": {},
   "source": [
    "## read_csv_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123179f-319a-448e-929f-dc31cc421fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_csv_from_gcs(project_id, bucket_name, file_path):\n",
    "  \"\"\"Reads a CSV file from a GCS bucket into a pandas DataFrame.\n",
    "\n",
    "  Args:\n",
    "    project_id: The Google Cloud project ID.\n",
    "    bucket_name: The name of the GCS bucket.\n",
    "    file_path: The path to the CSV file within the bucket.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame containing the CSV data.\n",
    "  \"\"\"\n",
    "\n",
    "  storage_client = storage.Client(project=project_id)\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob(file_path)\n",
    "\n",
    "  with blob.open('r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "  return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2c6a2-15fd-4aed-a5cd-b7abae70d1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check_categorical_columns\n",
    "\n",
    "def check_categorical_columns(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Check if any categorical columns contain numerical values or NaNs.\n",
    "    \"\"\"\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype != 'object':  # Check if the column is not of type 'object'\n",
    "            print(f\"Column {col} is not of type 'object'. It has type: {df[col].dtype}\")\n",
    "            print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "        elif df[col].isnull().any():  # Check if the column contains NaN values\n",
    "            print(f\"Column {col} contains NaN values.\")\n",
    "        else:\n",
    "            print(f\"Column {col} seems fine.\")\n",
    "        \n",
    "        # Check for numerical data in categorical columns\n",
    "        numerical_data = df[col][df[col].apply(lambda x: isinstance(x, (int, float)))]\n",
    "        if not numerical_data.empty:\n",
    "            print(f\"Column {col} contains numerical data: {numerical_data.unique()}\")\n",
    "\n",
    "# # List of categorical columns\n",
    "# categorical_cols = ['de_gender', 'de_maritalStatus', 'de_city', 'de_barangay', 'de_province',\n",
    "#                     'de_dependentsCount', 'de_subIndustryDescription', 'de_Education_type',\n",
    "#                     'deviceType', 'osversion_v2', 'brand', 'app_first_app_cat',\n",
    "#                     'app_last_app_cat', 'de_natureofwork_grouped']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbb12d-2e54-4e3f-b835-b2ba00c6125b",
   "metadata": {},
   "source": [
    "## load_pickle_from_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98073302-3f31-49eb-a44a-06cf6ec4d7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pickle_from_gcs(bucket_name, blob_path):\n",
    "    \"\"\"\n",
    "    Load pickle file from Google Cloud Storage\n",
    "    \n",
    "    Parameters:\n",
    "    bucket_name: Name of the GCS bucket\n",
    "    blob_path: Path to the blob in the bucket\n",
    "    \n",
    "    Returns:\n",
    "    Unpickled data\n",
    "    \"\"\"\n",
    "    from google.cloud import storage\n",
    "    import pickle\n",
    "    import io\n",
    "    \n",
    "    # Initialize GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_path)\n",
    "    \n",
    "    # Download blob content into memory\n",
    "    content = blob.download_as_bytes()\n",
    "    \n",
    "    # Load pickle data from memory\n",
    "    pickle_data = pickle.loads(content)\n",
    "    \n",
    "    return pickle_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15947b85-6022-401c-a3ad-32c7eb44c779",
   "metadata": {},
   "source": [
    "## Save the model joblib file - model_Good_Customer_Beta2 Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a7507-0150-4643-b017-7d1bf89273ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier  # Import both classes\n",
    "\n",
    "def save_model(model, save_dir, file_name):\n",
    "    \"\"\"\n",
    "    Save a CatBoost model to disk with proper validation and error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : CatBoostRegressor or CatBoostClassifier\n",
    "        The trained CatBoost model to save\n",
    "    save_dir : str\n",
    "        Directory path where the model should be saved\n",
    "    file_name : str\n",
    "        Name of the file to save the model as\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if save successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Validate model type\n",
    "    if not isinstance(model, (CatBoostRegressor, CatBoostClassifier)):\n",
    "        raise TypeError(\"Model must be a CatBoost model\")\n",
    "    \n",
    "    # Clean and validate the path\n",
    "    save_dir = os.path.expanduser(save_dir)  # Expand user directory if present\n",
    "    save_dir = os.path.abspath(save_dir)     # Convert to absolute path\n",
    "    \n",
    "    # Create full path\n",
    "    full_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Verify write permissions\n",
    "        if not os.access(save_dir, os.W_OK):\n",
    "            raise PermissionError(f\"No write permission for directory: {save_dir}\")\n",
    "            \n",
    "        # Save the model\n",
    "        joblib.dump(model, full_path)\n",
    "        \n",
    "        # Verify file was created\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"Model successfully saved to: {full_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"File was not created successfully\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9952c36-d80c-4aad-8c1e-dbf936a6d90f",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a00015-12be-41cd-85f0-b019d64da2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "with \n",
    "educate as \n",
    "(select distinct edu.digitalLoanAccountId, edu.education_id, edu1.description\n",
    "from `prj-prod-dataplatform.dl_loans_db_raw.tdbk_loan_purpose` edu\n",
    "inner join (select id, description from dl_loans_db_raw.tdbk_loan_lov_mtb where module = 'Education') edu1 on edu.education_id = edu1.id\n",
    "),\n",
    "educate2 as \n",
    "(select *, row_number() over(partition by digitalLoanAccountId order by education_id desc) rnk from educate),\n",
    "educate3 as \n",
    "(select * from educate2 where rnk = 1),\n",
    "base as \n",
    "(select \n",
    "b.customerId, b.onb_email, b.onb_email_verified_flag,\n",
    "       b.onb_place_of_birth, b.age, b.onb_latitude, b.onb_longitude,\n",
    "       b.onb_cnt_ongoing_loans, b.onb_tot_ongoing_loans_emi,\n",
    "       b.digitalLoanAccountId, b.loanAccountNumber, b.onboarding_datetime,\n",
    "       b.onb_mobile_no, b.loan_mobile_no, b.loan_alternate_mobile_no,\n",
    "       b.loan_purpose, b.loan_disbursementDateTime, b.loan_source_funds,\n",
    "       b.loan_source_funds_new, b.loan_employment_type,\n",
    "       b.loan_employment_type_new, b.loan_nature_of_work,\n",
    "       b.loan_nature_of_work_new, b.loan_industry_description,\n",
    "       b.loan_industry_description_new, b.loan_companyName,\n",
    "       b.loan_marital_status, b.loan_dependents_count,\n",
    "       b.loan_education_level, b.loan_ref_type1, b.loan_ref_type2,\n",
    "       b.loan_addressline, b.loan_province, b.loan_city, b.loan_barangay,\n",
    "       b.loan_postalcode, b.loan_geolocation, b.loan_docType,\n",
    "       b.loan_docNumber, b.loan_type, b.loan_product_type,\n",
    "       b.loan_osversion_v2, b.loan_brand, b.loan_self_dec_income,\n",
    "       b.loan_salary_scaled_income, b.loan_vas_opted_flag\n",
    ",\n",
    "CAST(\n",
    "            CASE \n",
    "                WHEN LOWER(b.loan_osversion_v2) LIKE 'android%' THEN \n",
    "                    -- Extract just the first number for android\n",
    "                    CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.loan_osversion_v2), r'android(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "                WHEN LOWER(b.loan_osversion_v2) LIKE 'ios%' THEN\n",
    "                    -- Extract just the first number for ios\n",
    "                    CAST(SPLIT(REGEXP_EXTRACT(LOWER(b.loan_osversion_v2), r'ios(.+)'), '.')[OFFSET(0)] AS FLOAT64)\n",
    "                ELSE \n",
    "                    CAST(SPLIT(b.loan_osversion_v2, '.')[OFFSET(0)] AS FLOAT64)\n",
    "            END AS FLOAT64\n",
    "        ) as clean_version,\n",
    "  CASE \n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2023-07-01' AND '2024-07-31' THEN 'Train'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-08-01' AND '2024-08-31' THEN 'Test'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-09-01' AND '2024-09-30' THEN 'OOT_SEP_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-10-01' AND '2024-10-31' THEN 'OOT_OCT_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-11-01' AND '2024-11-30' THEN 'OOT_NOV_24'\n",
    "    WHEN DATE_TRUNC(lmt.decision_date, DAY) BETWEEN '2024-12-01' AND '2024-12-31' THEN 'OOT_DEC_24'\n",
    "END AS Dataselection,\n",
    "lmt.disbursementDateTime,\n",
    "lmt.new_loan_type,\n",
    "lmt.Gender,\n",
    "from worktable_data_analysis.beta2_loan_details_jan2023_dec2024 b\n",
    "inner join `risk_credit_mis.loan_master_table` lmt on lmt.digitalLoanAccountid = b.digitalLoanAccountId\n",
    "left join educate3 on educate3.digitalLoanAccountId = b.digitalLoanAccountId\n",
    "where b.digitalLoanAccountId is not null\n",
    "and coalesce(lmt.Max_Ever_DPD, 0) < 10\n",
    "AND (upper(lmt.new_loan_type) like '%SIL%' or upper(lmt.new_loan_type) like '%QUICK%')\n",
    "AND DATE_TRUNC(lmt.termsAndConditionsSubmitDateTime, DAY) >= '2023-07-01'\n",
    "AND DATE(lmt.thirdDueDate) <= CURRENT_DATE()\n",
    "AND lmt.flagDisbursement = 1\n",
    "-- AND b.user_type in ('2_New Applicant', '1_Repeat Applicant')\n",
    ")\n",
    "select \n",
    "base.digitalLoanAccountId,\n",
    "base.loan_companyName,\n",
    "base.loan_product_type,\n",
    "base.loan_education_level,\n",
    "base.loan_industry_description_new industry_description,\n",
    "base.loan_employment_type_new employment_type,\n",
    "base.age,\n",
    "loan_city,\n",
    "base.loan_brand,\n",
    "base.loan_purpose,\n",
    "base.loan_osversion_v2 osversion_v2,\n",
    "base.clean_version,\n",
    "base.loan_docType,\n",
    "Gender,\n",
    "base.loan_dependents_count dependentsCount,\n",
    "base.loan_postalcode,\n",
    "base.loan_source_funds_new source_funds,\n",
    "base.loan_marital_status maritalStatus,\n",
    "base.loan_nature_of_work_new nature_of_work,\n",
    "base.loan_geolocation,\n",
    "base.onb_place_of_birth place_of_birth,\n",
    "base.onb_email email,\n",
    "case when cast(base.loan_self_dec_income as numeric) > 300000 then 300000 else cast(base.loan_self_dec_income as numeric) end as loan_monthly_income,\n",
    "base.Dataselection,\n",
    "base.onboarding_datetime,\n",
    "from base \n",
    ";\n",
    "\"\"\"\n",
    "data = client.query(sq).to_dataframe(progress_bar_type = 'tqdm')\n",
    "print(f\"The shape of the {MODELNAME}_{DATATYPE}_{VERSIONNAME}_{PRODUCT_TYPE} data is:\\t{data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecda9ef-06a9-4b69-bec9-39843731d2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccff498-ce05-4738-acdb-45c78668405f",
   "metadata": {},
   "source": [
    "## Save the Beta2V1_0_Step2 Production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4c523-6539-4959-8ddd-d745aac784b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILENAME = \"STEP2DATA_PRODUCTION\"\n",
    "step2datafilename = f\"{CURRENT_DATE}_{MODELNAME}{PRODUCT_TYPE}{VERSIONNAME}_{FILENAME}.csv\"\n",
    "print(step2datafilename)\n",
    "data.to_csv(f\"{LOCALPATH}_{step2datafilename}\", index = False)\n",
    "print(f\"The {step2datafilename} is saved at {LOCALPATH}_{step2datafilename} on {CURRENT_DATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e2a0a-a411-4bbb-994c-b3d24aac8384",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the data to cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210b65a-4b12-4544-8e0e-9ac5b3505516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindf = data[data['Dataselection'] == 'Train']\n",
    "testdf = data[data['Dataselection'] == 'Test']\n",
    "ootdfsep24 = data[data['Dataselection'] == 'OOT_SEP_24']\n",
    "ootdfoct24 = data[data['Dataselection'] == 'OOT_OCT_24']\n",
    "ootdfnov24 = data[data['Dataselection'] == 'OOT_NOV_24']\n",
    "ootdfdec24 = data[data['Dataselection'] == 'OOT_DEC_24']\n",
    "\n",
    "print(f\"The shape of overall dataset is:\\t {data.shape}\")\n",
    "print(f\"The shape of Train set is:\\t {traindf.shape}\")\n",
    "print(f\"The shape of Test set is:\\t {testdf.shape}\")\n",
    "print(f\"The shape of the OOT September 24 set is:\\t {ootdfsep24.shape}\")\n",
    "print(f\"The shape of the OOT October 24 set is:\\t {ootdfoct24.shape}\")\n",
    "print(f\"The shape of the OOT November 24 set is:\\t {ootdfnov24.shape}\")\n",
    "print(f\"The shape of the OOT December 24 set is:\\t {ootdfdec24.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69cc63-1a63-4ad1-989c-b09cf3f4c545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name = BUCKET_NAME\n",
    "filename = 'data'\n",
    "# Construct the new filename\n",
    "data_filename = f\"{CURRENT_DATE}_{MODELNAME}_{PRODUCT_TYPE}{VERSIONNAME}_{filename}.csv\"\n",
    "print(data_filename)\n",
    "destination_blob_name = f\"{CLOUDPATH_DATA}/{data_filename}\"\n",
    "save_df_to_gcs(data, bucket_name, destination_blob_name)\n",
    "data.to_csv(f\"{LOCALPATH}{data_filename}\", index = False)\n",
    "\n",
    "\n",
    "filename = 'traindf'\n",
    "# Construct the new filename\n",
    "new_filename = f\"{CURRENT_DATE}_{MODELNAME}_{PRODUCT_TYPE}{VERSIONNAME}_{filename}.csv\"\n",
    "print(new_filename)\n",
    "destination_blob_name = f\"{CLOUDPATH_DATA}/{new_filename}\"\n",
    "save_df_to_gcs(traindf, bucket_name, destination_blob_name)\n",
    "traindf.to_csv(f\"{LOCALPATH}{new_filename}\", index = False)\n",
    "traindf.to_csv(\"traindf.csv\", index = False)\n",
    "\n",
    "filename = 'testdf'\n",
    "new_filename = f\"{CURRENT_DATE}_{MODELNAME}_{PRODUCT_TYPE}{VERSIONNAME}_{filename}.csv\"\n",
    "print(new_filename)\n",
    "destination_blob_name = f\"{CLOUDPATH_DATA}/{new_filename}\"\n",
    "save_df_to_gcs(testdf, bucket_name, destination_blob_name)\n",
    "testdf.to_csv(f\"{LOCALPATH}{new_filename}\", index = False)\n",
    "testdf.to_csv(\"testdf.csv\", index = False)\n",
    "\n",
    "filename = 'ootdfsep24'\n",
    "new_filename = f\"{CURRENT_DATE}_{MODELNAME}_{PRODUCT_TYPE}{VERSIONNAME}_{filename}.csv\"\n",
    "print(new_filename)\n",
    "destination_blob_name = f\"{CLOUDPATH_DATA}/{new_filename}\"\n",
    "save_df_to_gcs(ootdfsep24, bucket_name, destination_blob_name)\n",
    "ootdfsep24.to_csv(f\"{LOCALPATH}{new_filename}\", index = False)\n",
    "ootdfsep24.to_csv(\"ootdfsep24.csv\", index = False)\n",
    "\n",
    "filename = 'ootdfoct24'\n",
    "new_filename = f\"{CURRENT_DATE}_{MODELNAME}_{PRODUCT_TYPE}{VERSIONNAME}_{filename}.csv\"\n",
    "print(new_filename)\n",
    "destination_blob_name = f\"{CLOUDPATH_DATA}/{new_filename}\"\n",
    "save_df_to_gcs(ootdfoct24, bucket_name, destination_blob_name)\n",
    "ootdfoct24.to_csv(f\"{LOCALPATH}{new_filename}\", index = False)\n",
    "ootdfoct24.to_csv(\"ootdfoct24.csv\", index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d2eb1-a932-4510-a95a-a3d1434e7149",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef810be-fcce-4a18-a72a-788a4e5df958",
   "metadata": {},
   "source": [
    "## Beta2 Step2 Production Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034288b5-44ad-481b-8e7e-ca5ee40e7879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gs://prod-asia-southeast1-tonik-aiml-workspace/Monthly_Income_Estimation/IncomeEstimation_Beta1V3.1/20250115_Beta1V3.1_Step2_dataStep2.csv\n",
    "project_id = 'prj-prod-dataplatform'\n",
    "bucket_name = BUCKET_NAME\n",
    "file_path = f'{CLOUDPATH_DATA}/{data_filename}'\n",
    "\n",
    "df = read_csv_from_gcs(project_id, bucket_name, file_path)\n",
    "print(f\"The shape of the {CLOUDPATH_DATA}/{data_filename} is :\\t {df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee057f-e117-4291-93be-9f3c6ff888b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cedf486-543d-4dd6-b174-ca27fc786279",
   "metadata": {},
   "source": [
    "# Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71915ee-c925-49ff-be72-ec1d0f15f605",
   "metadata": {},
   "source": [
    "## EmailFeatureTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c1b7e-a047-4a79-aee9-163cdf5e7789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmailFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.domain_freq = None\n",
    "        self.output_format = \"default\"  # Default output format\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'email' in X.columns:\n",
    "            X['email'].str.lower()\n",
    "            domains = X['email'].apply(lambda x: x.split('@')[1])\n",
    "            self.domain_freq = domains.value_counts().to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if 'email' not in X.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['domain'] = X_transformed['email'].apply(lambda x: x.split('@')[1])\n",
    "        X_transformed['tld'] = X_transformed['domain'].apply(lambda x: x.split('.')[-1])\n",
    "\n",
    "        # Return output based on the configured format\n",
    "        if self.output_format == \"pandas\":\n",
    "            return X_transformed[['domain', 'tld']]\n",
    "        else:\n",
    "            return X_transformed[['domain', 'tld']].to_numpy()\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return ['domain', 'tld']\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Set the output format of the transformer.\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcd9a7-f91b-4637-aa83-5034d90eff64",
   "metadata": {},
   "source": [
    "## VersionCategorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0a682-38b1-467f-9e36-16ee7756dc76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VersionCategorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.output_format = \"default\"  # Default output format\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['osversionType'] = X.apply(self._categorize, axis=1)\n",
    "\n",
    "        # Return output based on the configured format\n",
    "        if self.output_format == \"pandas\":\n",
    "            return X[['osversionType']]\n",
    "        else:\n",
    "            return X[['osversionType']].to_numpy()\n",
    "\n",
    "    def _categorize(self, row):\n",
    "        os_version = str(row['osversion_v2']).lower()\n",
    "        clean_version = row['clean_version']\n",
    "\n",
    "        if 'android' in os_version:\n",
    "            if clean_version >= 15:\n",
    "                return '5-Pioneer'\n",
    "            elif clean_version == 14:\n",
    "                return '4-Innovator'\n",
    "            elif clean_version == 7:\n",
    "                return '3-ComfortSeeker'\n",
    "            elif clean_version in [13, 12, 11, 10, 6]:\n",
    "                return '2-Voyager'\n",
    "        elif 'ios' in os_version:\n",
    "            if clean_version >= 18:\n",
    "                return '5-Pioneer'\n",
    "            elif clean_version in [17, 16]:\n",
    "                return '3-ComfortSeeker'\n",
    "            elif clean_version in [15, 14]:\n",
    "                return '2-Voyager'\n",
    "        return '1-Nomad'\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return ['osversionType']\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Set the output format of the transformer.\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47d913-8ff4-466f-9bbc-ce9744eeef4b",
   "metadata": {},
   "source": [
    "## NatureOfWorkTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1264f-bb69-44d3-9f7c-7e683987d6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NatureOfWorkTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=50):\n",
    "        self.threshold = threshold\n",
    "        self.mapping = None\n",
    "        self.column_present = False\n",
    "        self.output_format = \"default\"  # Default output format\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'nature_of_work' in X.columns:\n",
    "            self.column_present = True\n",
    "            value_counts = X['nature_of_work'].value_counts()\n",
    "            self.mapping = {work: 'Others' if count < self.threshold else work \n",
    "                          for work, count in value_counts.items()}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if not self.column_present or 'nature_of_work' not in X.columns:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['nature_of_work_grouped'] = X_transformed['nature_of_work'].map(self.mapping)\n",
    "\n",
    "        # Return output based on the configured format\n",
    "        if self.output_format == \"pandas\":\n",
    "            return X_transformed[['nature_of_work_grouped']]\n",
    "        else:\n",
    "            return X_transformed[['nature_of_work_grouped']].to_numpy()\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return ['nature_of_work_grouped']\n",
    "\n",
    "    def set_output(self, transform=\"default\"):\n",
    "        \"\"\"\n",
    "        Set the output format of the transformer.\n",
    "        \"\"\"\n",
    "        self.output_format = transform\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d45f12-6250-484b-af96-a976d13eadd6",
   "metadata": {},
   "source": [
    "## Create Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d4840-b1ca-4050-94ee-9c48cfa85252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(df):\n",
    "    # Existing transformers\n",
    "    email_transformer = Pipeline([\n",
    "        ('email_features', EmailFeatureTransformer())\n",
    "    ])\n",
    "\n",
    "    version_transformer = Pipeline([\n",
    "        ('version_cat', VersionCategorizer())\n",
    "    ])\n",
    "\n",
    "    work_transformer = Pipeline([\n",
    "        ('work_features', NatureOfWorkTransformer(threshold=50))\n",
    "    ])\n",
    "\n",
    "#     text_transformer = Pipeline([\n",
    "#         ('text_cluster', TextClusteringTransformer(\n",
    "#             col1='loan_company_name', \n",
    "#             col2='industry_description', \n",
    "#             n_clusters=6\n",
    "#         ))\n",
    "#     ])\n",
    "    \n",
    "#     datetime_transformer = Pipeline([\n",
    "#         ('datetime_features', DateTimeFeatureTransformer())\n",
    "#     ])\n",
    "    \n",
    "#     telecom_transformer = Pipeline([\n",
    "#         ('telecom_provider', TelecomProviderTransformer())\n",
    "#     ])\n",
    "    \n",
    "#     fuzzy_match_transformer = Pipeline([\n",
    "#         ('fuzzy_match', FuzzyMatchTransformer(col1='place_of_birth', col2='onb_city', threshold=50))\n",
    "#     ])\n",
    "    \n",
    "#     document_os_transformer = Pipeline([\n",
    "#         ('document_os_features', DocumentTypeOSVersionTransformer())\n",
    "#     ])\n",
    "    \n",
    "#     fuzzy_name_email_transformer = Pipeline([\n",
    "#         ('fuzzy_name_email_match', FuzzyNameEmailMatcher(threshold=80))\n",
    "#     ])\n",
    "    \n",
    "    transformers = []\n",
    "\n",
    "    if ('osversion_v2' in df.columns) and ('clean_version' in df.columns):\n",
    "        transformers.append(('version', version_transformer, ['osversion_v2', 'clean_version']))\n",
    "\n",
    "    if 'email' in df.columns:\n",
    "        transformers.append(('email', email_transformer, ['email']))\n",
    "\n",
    "    # if 'onboarding_datetime' in df.columns:\n",
    "    #     transformers.append(('datetime', datetime_transformer, ['onboarding_datetime']))\n",
    "        \n",
    "    if 'nature_of_work' in df.columns:\n",
    "        transformers.append(('work', work_transformer, ['nature_of_work']))\n",
    "        \n",
    "#     if ('loan_company_name' in df.columns) and ('industry_description' in df.columns):\n",
    "#         transformers.append(('text', text_transformer, ['loan_company_name', 'industry_description']))\n",
    "        \n",
    "#     if 'onb_mobile_no' in df.columns:\n",
    "#         transformers.append(('telecom', telecom_transformer, ['onb_mobile_no']))\n",
    "    \n",
    "#     if ('place_of_birth' in df.columns) and ('onb_city' in df.columns):\n",
    "#         transformers.append(('fuzzy_match', fuzzy_match_transformer, ['place_of_birth', 'onb_city']))\n",
    "        \n",
    "#     if ('onb_document_type' in df.columns) and ('osversion_v2' in df.columns):\n",
    "#         transformers.append(('document_os', document_os_transformer, ['onb_document_type', 'osversion_v2']))\n",
    "        \n",
    "#     if ('first_name' in df.columns) and ('middle_name' in df.columns) and ('last_name' in df.columns) and ('email' in df.columns):\n",
    "#         transformers.append(('fuzzy_name_email', fuzzy_name_email_transformer, ['first_name','middle_name', 'last_name', 'email']))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    # Replace missing values with 'Missing'\n",
    "    categorical_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "\n",
    "    # Create the final pipeline\n",
    "    final_pipeline = Pipeline([\n",
    "        ('categorical_imputer', categorical_imputer),\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "\n",
    "    # Set the output to pandas DataFrame\n",
    "    final_pipeline.set_output(transform=\"pandas\")\n",
    "\n",
    "    return final_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2100706-5dd1-44e2-8f0e-fa287e5f4841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abeaaf67-2f8b-4d5c-aa94-bae8bf218357",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test the entire transformes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db22f4a-3d90-4a1b-8ed1-1a34e9b660a6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb9555-c301-4e2a-ac03-4b55b7908047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, numerical_cols, categorical_cols = prepare_data_cat_model1(dfmerged, 'loan_monthly_income')\n",
    "\n",
    "# Assuming dfmerged is your original DataFrame\n",
    "preprocessor = create_preprocessor(df)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Inspect the results\n",
    "print(\"Transformed dataframe shape:\", df_transformed.shape)\n",
    "print(\"\\nColumns in transformed dataframe:\")\n",
    "print(df_transformed.columns)\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "print(\"\\nTransformed dataframe head:\")\n",
    "# Now combine the original dfmerged with df_transformed\n",
    "# First, get the list of columns that were used in transformations to avoid duplicates\n",
    "transformed_source_cols = ['email', 'osversion_v2', 'clean_version', 'nature_of_work', \n",
    "                           ]\n",
    "\n",
    "# Drop the source columns from dfmerged to avoid duplicates\n",
    "dfmerged_filtered = df.drop(columns=transformed_source_cols)\n",
    "\n",
    "# Combine the filtered original dataframe with the transformed features\n",
    "final_df = pd.concat([dfmerged_filtered, df_transformed], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627d421-223d-41d2-8e0d-55159cbd285d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df.rename(columns = {'version__osversionType':'ln_os_versiontype', \n",
    "                           'work__nature_of_work_grouped':'ln_nature_of_work',\n",
    "                          'maritalStatus':'ln_marital_status',\n",
    "                           'loan_product_type':'ln_loan_type',\n",
    "                           'loan_education_level':'ln_education_level',\n",
    "                           'industry_description':'ln_Industry_desc',\n",
    "                           'employment_type': 'ln_Employment_type',\n",
    "                           'age': 'ln_age',\n",
    "                           'loan_city': 'ln_city',\n",
    "                           'loan_purpose': 'ln_purpose_desc',\n",
    "                           'loan_docType': 'ln_docType',\n",
    "                           'Gender': 'ln_gender',\n",
    "                           'dependentsCount': 'ln_dependent_count',\n",
    "                           'loan_postalcode':'ln_postalcode',\n",
    "                           'source_funds': 'ln_source_funds',\n",
    "                           'loan_geolocation': 'ln_geolocation',\n",
    "                           'place_of_birth': 'ln_place_of_birth',\n",
    "                           'loan_monthly_income': 'ln_monthly_income',\n",
    "                           'onboarding_datetime':'onb_datetime',\n",
    "                           'version__osversionType': 'ln_osversion_type',\n",
    "                           'email__domain': 'ln_email_domain',\n",
    "                           'email__tld': 'ln_email_tld',\n",
    "                           'loan_brand':'ln_brand'\n",
    "                            }, inplace = True)\n",
    "\n",
    "final_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4d7d3-f312-4e25-b08c-ef9d93d85b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "['Gender', 'dependentsCount', 'device_os_version', 'employment_type', 'industry_description', 'loan_brand', 'loan_city', 'loan_docType', 'loan_education_level', 'loan_product_type', 'loan_purpose', 'source_funds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a159e5-149f-488e-bbc7-0479c7a9664f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "['age', 'encoded_company_name_group', 'loan_postalcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a24004-aef2-45b5-9375-0a4bd96beb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = final_df[['digitalLoanAccountId', 'loan_companyName','ln_monthly_income','ln_gender', 'ln_dependent_count', 'ln_osversion_type',\n",
    "              'ln_Employment_type','ln_Industry_desc', 'ln_brand', 'ln_city', 'ln_docType', 'ln_education_level', 'ln_loan_type',\n",
    "               'ln_purpose_desc', 'ln_source_funds', 'ln_age', 'ln_postalcode' , 'Dataselection' ]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af4c84-c5cb-44e4-83fa-348a45f7d386",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8ff44-b7ad-4ca1-9441-16d0917fdc98",
   "metadata": {},
   "source": [
    "## Custom_Catboost_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d5f4f-4c0b-4e65-9a86-addd2631ce9c",
   "metadata": {},
   "source": [
    "## model_Good_Customer_Beta2 V1_0  Step2 Production with 15 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ba2b6-dd9b-4e5b-a2cd-1af1a36614f8",
   "metadata": {},
   "source": [
    "### Model Running Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcbc09-819a-4944-92d2-454b6ef07ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Custom_Catboost_Model(df, target_variable, threshold=0.8, columns_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Custom CatBoost model implementation with data preparation and visualization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    target_variable : str\n",
    "        Name of the target variable column\n",
    "    threshold : float, optional\n",
    "        Threshold for multicollinearity detection (default: 0.8)\n",
    "    columns_to_exclude : list, optional\n",
    "        List of columns to exclude from the analysis\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from catboost import CatBoostRegressor\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "    \n",
    "    # Set default columns to exclude if none provided\n",
    "    if columns_to_exclude is None:\n",
    "        columns_to_exclude = ['cust_id', 'digitalLoanAccountId', 'onboarding_datetime', 'loanAccountNumber']\n",
    "\n",
    "    def plot_actual_vs_predicted(y_true, y_pred, title='Actual vs Predicted Values'):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_residuals_hist(y_true, y_pred, title='Residual Histogram Plot'):\n",
    "        y_true_float = np.array(y_true, dtype=float)\n",
    "        y_pred_float = np.array(y_pred, dtype=float)\n",
    "        residuals = y_true_float - y_pred_float\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title('Residual Histogram')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_lift_chart(y_test, y_pred, n_bins=10):\n",
    "        results = pd.DataFrame({\n",
    "            'Actual': y_test,\n",
    "            'Predicted': y_pred\n",
    "        })\n",
    "        results['Decile'] = pd.qcut(results['Predicted'], q=n_bins, labels=False)\n",
    "        lift_chart_data = results.groupby('Decile').agg(\n",
    "            Avg_Predicted=('Predicted', 'mean'),\n",
    "            Avg_Actual=('Actual', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(lift_chart_data['Avg_Predicted'], label='Predicted', marker='o')\n",
    "        plt.plot(lift_chart_data['Avg_Actual'], label='Actual', marker='s')\n",
    "        plt.title(\"Lift Chart\")\n",
    "        plt.xlabel(f\"Decile (1-{n_bins})\")\n",
    "        plt.ylabel(\"Average Value\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_gain_chart(y_test, y_pred, n_bins=10):\n",
    "        results = pd.DataFrame({\n",
    "            'Actual': y_test,\n",
    "            'Predicted': y_pred\n",
    "        })\n",
    "        results = results.sort_values(by='Predicted', ascending=False).reset_index(drop=True)\n",
    "        results['Cumulative_Actual'] = results['Actual'].cumsum()\n",
    "        results['Cumulative_Predicted'] = results['Predicted'].cumsum()\n",
    "        results['Cumulative_Actual_Percent'] = results['Cumulative_Actual'] / results['Actual'].sum() * 100\n",
    "        results['Cumulative_Predicted_Percent'] = results['Cumulative_Predicted'] / results['Predicted'].sum() * 100\n",
    "        results['Percentage_of_Data'] = np.linspace(1 / len(results), 1, len(results)) * 100\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(results['Percentage_of_Data'], results['Cumulative_Predicted_Percent'], label='Predicted', marker='o')\n",
    "        plt.plot(results['Percentage_of_Data'], results['Cumulative_Actual_Percent'], label='Actual', marker='s')\n",
    "        plt.title(\"Gain Chart\")\n",
    "        plt.xlabel(\"Percentage of Data\")\n",
    "        plt.ylabel(\"Cumulative Percentage\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def categorize_columns(df, target_variable):\n",
    "        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        numerical_cols = [col for col in numerical_cols if col != target_variable]\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        categorical_cols = [col for col in categorical_cols if col != target_variable]\n",
    "        return numerical_cols, categorical_cols\n",
    "\n",
    "    def remove_multicollinearity(df, numerical_cols, threshold=0.8):\n",
    "        corr_matrix = df[numerical_cols].corr().abs()\n",
    "        # Sort columns to ensure consistent ordering\n",
    "        corr_matrix = corr_matrix.sort_index(axis=0).sort_index(axis=1)\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        # Sort to ensure consistent column dropping\n",
    "        to_drop = sorted([column for column in upper_tri.columns if any(upper_tri[column] > threshold)])\n",
    "        reduced_cols = sorted([col for col in numerical_cols if col not in to_drop])\n",
    "        return reduced_cols, to_drop\n",
    "    \n",
    "    def load_dataframe_from_gcs(bucket_name, blob_name):\n",
    "        \"\"\"Loads a DataFrame from a pickle file stored in Google Cloud Storage.\n",
    "\n",
    "        Args:\n",
    "            bucket_name: The name of the GCS bucket.\n",
    "            blob_name: The name of the blob (file) within the bucket.\n",
    "\n",
    "        Returns:\n",
    "            A pandas DataFrame, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            storage_client = storage.Client()\n",
    "            bucket = storage_client.bucket(bucket_name)\n",
    "            blob = bucket.blob(blob_name)\n",
    "\n",
    "            # Download the blob to a BytesIO object\n",
    "            blob_as_bytes = blob.download_as_bytes()\n",
    "\n",
    "            # Load the DataFrame from the BytesIO object\n",
    "            df = pickle.loads(blob_as_bytes)\n",
    "\n",
    "            # Check if loaded object is a pandas DataFrame\n",
    "            if not isinstance(df, pd.DataFrame):\n",
    "                raise TypeError(f\"Object loaded from pickle file is not a pandas DataFrame. It is a {type(df)}.\")\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading DataFrame: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def reduce_doc_type(doc_type):\n",
    "        # Define mappings from specific document types to common categories\n",
    "        doc_type_mappings = {\n",
    "            'Id Card': 'ID Card',\n",
    "            'UMID Card': 'UMID Card',\n",
    "            'Driving License': 'Driving License',\n",
    "            'ePassport': 'Passport',\n",
    "            'Professional Id Card': 'Professional ID Card',\n",
    "            'Social Security Card': 'Social Security Card',\n",
    "            'Postal Id Card': 'Postal ID Card',\n",
    "            'Professional Driving License': 'Driving License',\n",
    "            'Passport': 'Passport'\n",
    "        }\n",
    "\n",
    "        # Iterate through the mappings and return the common category if a match is found\n",
    "        for key, value in doc_type_mappings.items():\n",
    "            if key in doc_type:\n",
    "                return value\n",
    "\n",
    "        # Return 'Other' if no match is found\n",
    "        return 'Other'\n",
    "    \n",
    "    from google.cloud import storage\n",
    "    import joblib\n",
    "    import tempfile\n",
    "    import os\n",
    "\n",
    "    def load_dataframe_from_gcs_joblib(bucket_name, blob_name):\n",
    "        \"\"\"\n",
    "        Load a DataFrame from Google Cloud Storage using joblib\n",
    "\n",
    "        Parameters:\n",
    "        bucket_name (str): Name of the GCS bucket\n",
    "        blob_name (str): Path to the blob in GCS\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: The loaded DataFrame\n",
    "        \"\"\"\n",
    "        # Create a client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Get bucket and blob\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "\n",
    "        # Create a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "            # Download the blob to the temporary file\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "\n",
    "            # Load the data using joblib\n",
    "            data = joblib.load(temp_file.name)\n",
    "\n",
    "        # Clean up the temporary file\n",
    "        os.unlink(temp_file.name)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # # Example usage:\n",
    "    # bucket_name = \"prod-asia-southeast1-tonik-aiml-workspace\"\n",
    "    # blob_name = \"Monthly_Income_Estimation/Data/Beta1_Model/Supporting_Data/Artifacts/20250106_data_GoodCustomer_Beta2.pkl\"\n",
    "    # dfencodeddata = load_dataframe_from_gcs(bucket_name, blob_name)\n",
    "    # print(dfencodeddata.columns)\n",
    "    #gs://prod-asia-southeast1-tonik-aiml-workspace/Monthly_Income_Estimation/Target_Encoded_Artifacts/CompanyNameTargetEncoderfull.joblib\n",
    "    \n",
    "\n",
    "    \n",
    "    bucket_name = BUCKET_NAME\n",
    "    blob_name = \"Monthly_Income_Estimation/Target_Encoded_Artifacts/companytrgencode.joblib\"\n",
    "    # blob_name = f\"{CLOUDPATH}/companytrgencode.joblib\"\n",
    "    # print(f\"Company Encoder downloaded from {CLOUDPATH}\")\n",
    "    print(f\"Data downloaded from {blob_name}\")\n",
    "\n",
    "    dfencodeddata = load_dataframe_from_gcs_joblib(bucket_name, blob_name)\n",
    "    print(dfencodeddata.columns)\n",
    "    \n",
    "    # Modified prepare_data_cat_model1 function\n",
    "    def prepare_data_cat_model1(df, target_variable):\n",
    "        # Sort the input DataFrame by index\n",
    "        df = df.sort_index().copy()\n",
    "        \n",
    "            # Ensure consistent data types\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = df[col].astype('float64')\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype('int64')\n",
    "        \n",
    "        # Store digitalLoanAccountId before dropping\n",
    "        train_loan_ids = df[df['Dataselection'] == 'Train']['digitalLoanAccountId']\n",
    "        test_loan_ids = df[df['Dataselection'] == 'Test']['digitalLoanAccountId']\n",
    "        \n",
    "        # [Rest of the data preparation code remains the same until the return statement]\n",
    "        \n",
    "        if 'ln_docType' in df.columns:\n",
    "            df['ln_docType'] = df['ln_docType'].apply(reduce_doc_type)\n",
    "\n",
    "#         if 'de_onboard_document_type' in df.columns:\n",
    "#             df['de_onboard_document_type'] = df['de_onboard_document_type'].apply(reduce_doc_type)\n",
    "        \n",
    "        dfcn = dfencodeddata[['loan_company_name', 'freq_encodedcompanyName', 'target_encodedcompanyName', 'encoded_company_name_group']].copy()\n",
    "        print(f\"The shape of dfcn before duplicate drop is:\\t {dfcn.shape}\")\n",
    "        dfcn = dfcn.drop_duplicates()\n",
    "        print(f\"The shape of dfcn is:\\t {dfcn.shape}\")\n",
    "        dfcn['freq_encodedcompanyName'] = dfcn['freq_encodedcompanyName'].fillna(1)\n",
    "        dfcn['target_encodedcompanyName'] = dfcn['target_encodedcompanyName'].fillna(dfencodeddata['global_mean'])\n",
    "        dfcn['encoded_company_name_group'] = dfcn['encoded_company_name_group'].fillna(dfencodeddata['global_mean'])\n",
    "        df = df.merge(dfcn, left_on ='loan_companyName', right_on ='loan_company_name', how='left')\n",
    "        df.drop(columns = ['loan_company_name', 'loan_companyName'], inplace = True)\n",
    "        \n",
    "        df_train = df[df['Dataselection'] == 'Train']\n",
    "        df_test = df[df['Dataselection'] == 'Test']\n",
    "        df_train = df_train.drop(columns=['Dataselection'])\n",
    "        df_test = df_test.drop(columns=['Dataselection'])\n",
    "        \n",
    "        df_train = df_train.sort_index()\n",
    "        df_test = df_test.sort_index()\n",
    "        \n",
    "        print(f\"The shape of df_train is:\\t{df_train.shape}\")\n",
    "        \n",
    "        # Remove specified columns except digitalLoanAccountId\n",
    "        # temp_exclude = [col for col in columns_to_exclude if col != 'digitalLoanAccountId']\n",
    "        temp_exclude = [col for col in columns_to_exclude]\n",
    "        print(temp_exclude)\n",
    "        df_train = df_train.drop(columns=temp_exclude, errors='ignore')\n",
    "        df_test = df_test.drop(columns=temp_exclude, errors='ignore')\n",
    "        \n",
    "        numerical_cols, categorical_cols = categorize_columns(df_train, target_variable)\n",
    "        # Sort the numerical cols and Categorical cols\n",
    "        numerical_cols = sorted(numerical_cols)\n",
    "        categorical_cols = sorted(categorical_cols)\n",
    "        numerical_cols, dropped_cols = remove_multicollinearity(df_train, numerical_cols)\n",
    "        print(\"Dropped columns due to multicollinearity:\", dropped_cols)\n",
    "        \n",
    "        X_train = df_train[numerical_cols + categorical_cols]\n",
    "        y_train = df_train[target_variable]\n",
    "        X_test = df_test[numerical_cols + categorical_cols]\n",
    "        y_test = df_test[target_variable]\n",
    "        \n",
    "        X_train = X_train.sort_index()\n",
    "        X_test = X_test.sort_index()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            X_train[col] = X_train[col].fillna('missing').astype(str)\n",
    "            X_test[col] = X_test[col].fillna('missing').astype(str)\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)\n",
    "            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, numerical_cols, categorical_cols, train_loan_ids, test_loan_ids\n",
    "\n",
    "    def evaluate_model(y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "        print(\"Model Performance Metrics:\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "        print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}\")\n",
    "        return mae, mse, rmse, r2, mape\n",
    "\n",
    "    def get_feature_importance(model, numerical_cols, categorical_cols):\n",
    "        all_features = numerical_cols + categorical_cols\n",
    "        feature_importance = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': all_features,\n",
    "            'importance': feature_importance\n",
    "        })\n",
    "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        return feature_importance_df\n",
    "\n",
    "    # Main execution\n",
    "    print(\"Preparing data...\")\n",
    "    X_train, X_test, y_train, y_test, numerical_cols, categorical_cols, train_loan_ids, test_loan_ids = prepare_data_cat_model1(\n",
    "        df, target_variable\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining model...\")\n",
    "    cat_features_indices = [X_train.columns.get_loc(col) for col in categorical_cols]\n",
    "    # model = CatBoostRegressor(\n",
    "    #     cat_features=cat_features_indices,\n",
    "    #     iterations=1000,\n",
    "    #     learning_rate=0.05,\n",
    "    #     l2_leaf_reg=8,\n",
    "    #     random_strength=1,\n",
    "    #     random_seed=42,\n",
    "    #     loss_function='MAE',\n",
    "    #     depth=8,\n",
    "    #     verbose=False\n",
    "    # )\n",
    "    \n",
    "    # model = CatBoostRegressor(\n",
    "    #     cat_features=cat_features_indices,\n",
    "    #     iterations=1000,\n",
    "    #     learning_rate=0.05,\n",
    "    #     l2_leaf_reg=8,\n",
    "    #     random_seed=random_state,\n",
    "    #     # random_seed=42,  # Keep only this one\n",
    "    #     loss_function='MAE',\n",
    "    #     depth=8,\n",
    "    #     verbose=False,\n",
    "    #     bootstrap_type='No',\n",
    "    #     # bootstrap_type='Bernoulli',\n",
    "    #     subsample=0.8,\n",
    "    #     # random_state=42,  # Remove this\n",
    "    #     boosting_type='Plain',\n",
    "    #     thread_count=1,\n",
    "    #     train_dir='catboost_info',\n",
    "    #     min_data_in_leaf=1,\n",
    "    #     leaf_estimation_iterations=1\n",
    "    #     )\n",
    "    \n",
    "    # Modify your CatBoostRegressor parameters:\n",
    "    # model = CatBoostRegressor(\n",
    "    #     cat_features=cat_features_indices,\n",
    "    #     iterations=1000,\n",
    "    #     learning_rate=0.05,\n",
    "    #     l2_leaf_reg=8,\n",
    "    #     random_seed=random_state,\n",
    "    #     loss_function='MAE',\n",
    "    #     depth=8,\n",
    "    #     verbose=False,\n",
    "    #     bootstrap_type='No',        # Change this\n",
    "    #     boosting_type='Plain',\n",
    "    #     thread_count=1,             # Add this\n",
    "    #     train_dir='catboost_info',  # Add this\n",
    "    #     leaf_estimation_iterations=1\n",
    "    # )\n",
    "    \n",
    "    model = CatBoostRegressor(\n",
    "        cat_features=cat_features_indices,\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        l2_leaf_reg=8,\n",
    "        random_seed=42,\n",
    "        loss_function='MAE',\n",
    "        depth=8,\n",
    "        verbose=False,\n",
    "        bootstrap_type='No',\n",
    "        boosting_type='Plain',\n",
    "        thread_count=1,\n",
    "        train_dir='catboost_info',\n",
    "        leaf_estimation_iterations=1,\n",
    "        has_time=False,           # Add this\n",
    "        allow_const_label=True,   # Add this\n",
    "        score_function='L2'       # Add this\n",
    "        )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Create result dataframes\n",
    "    train_result_df = pd.DataFrame({\n",
    "        'digitalLoanAccountId': train_loan_ids,\n",
    "        'actual_value': y_train,\n",
    "        'predicted_value': y_pred_train\n",
    "    })\n",
    "\n",
    "    test_result_df = pd.DataFrame({\n",
    "        'digitalLoanAccountId': test_loan_ids,\n",
    "        'actual_value': y_test,\n",
    "        'predicted_value': y_pred_test\n",
    "    })\n",
    "\n",
    "    # Combine train and test results\n",
    "    result_df = pd.concat([train_result_df, test_result_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    acc_train = model.score(X_train, y_train) * 100\n",
    "    acc_test = model.score(X_test, y_test) * 100\n",
    "    print(f'Training accuracy: {acc_train:.2f}%')\n",
    "    print(f'Test accuracy: {acc_test:.2f}%')\n",
    "\n",
    "    metrics = evaluate_model(y_test, y_pred_test)\n",
    "    feature_importance_df = get_feature_importance(model, numerical_cols, categorical_cols)\n",
    "    \n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    \n",
    "    print(\"\\nGenerating plots...\")\n",
    "    plot_actual_vs_predicted(y_test, y_pred_test)\n",
    "    plot_residuals_hist(y_test, y_pred_test)\n",
    "    plot_lift_chart(y_test, y_pred_test)\n",
    "    plot_gain_chart(y_test, y_pred_test)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance_df.head(20))\n",
    "    \n",
    "    shap.plots.beeswarm(shap_values, max_display=20)\n",
    "    shap.plots.bar(shap_values, max_display=20)\n",
    "    \n",
    "    shap_values_array = shap_values.values\n",
    "    shap_sum = np.abs(shap_values_array).mean(axis=0)\n",
    "    feature_importance_shap = pd.DataFrame(list(zip(X_test.columns, shap_sum)), columns=['feature', 'shap_importance'])\n",
    "    top_shap_features = feature_importance_shap.sort_values(by='shap_importance', ascending=False).head(20)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'feature_importance': feature_importance_df,\n",
    "        'predictions': y_pred_test,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'numerical_cols': numerical_cols,\n",
    "        'categorical_cols': categorical_cols,\n",
    "        'top_ten_shap_features': top_shap_features,\n",
    "        'result_df': result_df  # Added result_df to the return dictionary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccc216-2bce-452a-a85b-ac6df3250945",
   "metadata": {},
   "source": [
    "## Overall Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f0837-c4ab-4373-9e25-0e516e96f24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your columns to exclude (optional, there's a default list in the function)\n",
    "columns_to_exclude = ['cust_id', 'digitalLoanAccountId', 'onboarding_datetime','document_valid_flag','mothersMaidenFullName', 'loan_company_name', 'email', 'text__company_and_industry', \n",
    "                     'onb_latitude',   'onb_longitude',  'user_type', 'loan_docNumber',  'Dataselection', 'loanAccountNumber', 'text__cluster', 'onb_city' , 'loan_province', 'loan_type'\n",
    "                      , 'freq_encodedcompanyName',  'onb_PermanentAddress', 'target_encodedcompanyName', 'datetime__Onboarding_week_of_month', 'datetime__Onboarding_day_of_week', 'datetime__Onboarding_mon_of_year'\n",
    "                       ]\n",
    "\n",
    "# Run the model\n",
    "results = Custom_Catboost_Model(df, 'ln_monthly_income', threshold=0.8, columns_to_exclude=columns_to_exclude)\n",
    "results_Good_customer = results.copy()\n",
    "# Access results\n",
    "model = results['model']\n",
    "metrics = results['metrics']\n",
    "feature_importance = results['feature_importance']\n",
    "predictions = results['predictions']\n",
    "X_train_data = results['X_train']\n",
    "X_test_data = results['X_test']\n",
    "y_train_data = results['y_train']\n",
    "y_test_data = results['y_test']\n",
    "numerical_cols = results['numerical_cols']\n",
    "categorical_cols = results['categorical_cols']\n",
    "top_ten_shap_features = results['top_ten_shap_features']\n",
    "result_df = results['result_df']\n",
    "\n",
    "print(f\"Top Ten Shap features:/t {top_ten_shap_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e6884-8273-4e5b-977e-dcb289e6eca2",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bff70-c707-4a73-a972-1ca4428a61a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save the model joblib file - model_Good_Customer_Top15SHAPFeatures_Beta1V3_Step2\n",
    "\n",
    "MODELTOP15FILENAME = f\"model_Good_Customer_Top15SHAPFeatures_{MODELNAME}_{DATATYPE}_{VERSIONNAME}.joblib\"\n",
    "print(MODELTOP15FILENAME)\n",
    "destination_blob_name = f\"{CLOUDPATH_ARTIFACTS}/{MODELTOP15FILENAME}\"\n",
    "save_model(model,  f\"{LOCALPATH}\", f\"{MODELTOP15FILENAME}\")\n",
    "\n",
    "result_filename = f\"result_df_Top15SHAP_{MODELNAME}_{DATATYPE}_{VERSIONNAME}\"\n",
    "result_df.to_csv(f\"{LOCALPATH}/{result_filename}.csv\")\n",
    "\n",
    "numcoldf = pd.DataFrame(numerical_cols, columns=['numerical_cols'])\n",
    "numfilename = f\"Numericalcols_Top15SHAP_{MODELNAME}_{DATATYPE}_{VERSIONNAME}\"\n",
    "numcoldf.to_csv(f\"{LOCALPATH}/{numfilename}.csv\")\n",
    "\n",
    "catcoldf = pd.DataFrame(categorical_cols, columns=['categorical_cols'])\n",
    "catfilename = f\"Categorical_cols_top15SHAPP_{MODELNAME}_{DATATYPE}_{VERSIONNAME}\"\n",
    "catcoldf.to_csv(f\"{LOCALPATH}/{catfilename}.csv\")\n",
    "\n",
    "topshapdf = pd.DataFrame(top_ten_shap_features, columns=['top_ten_shap_features'])\n",
    "topshapfilename = f\"top_ten_shap_features_top15SHAP{MODELNAME}_{DATATYPE}_{VERSIONNAME}\"\n",
    "topshapdf.to_csv(f\"{LOCALPATH}/{topshapfilename}.csv\")\n",
    "\n",
    "# Serialize the model using joblib\n",
    "joblib.dump(model, f'{MODELTOP15FILENAME}')\n",
    "\n",
    "# Upload to Google Cloud Storage\n",
    "client = storage.Client()\n",
    "bucket_name = BUCKET_NAME\n",
    "source_file_name = MODELTOP15FILENAME\n",
    "destination_blob_name = f\"{CLOUDPATH_ARTIFACTS}/{MODELTOP15FILENAME}\"\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(destination_blob_name)\n",
    "blob.upload_from_filename(source_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335ba58-8e17-4858-baa5-90bb01af538f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "incomeestimation_env",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "IncomeEstimation_env",
   "language": "python",
   "name": "incomeestimation_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
